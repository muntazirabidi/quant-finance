# Question: Normal Variables and Independence: Correlation vs Dependence

Let $X \sim \text{Normal}(0,1)$ and $Y \sim \text{Normal}(0,1)$. If the correlation coefficient is $\rho_{XY} = 0$:

1. Are $X$ and $Y$ independent?
2. Give some examples where $X$ and $Y$ are in fact dependent, but where zero correlation still holds.

## Solution

### Part 1: Zero Correlation vs Independence

For normal random variables, zero correlation ($\rho_{XY} = 0$) does imply independence. This is a special property of the multivariate normal distribution.

However, this implication does not hold in general for other distributions. Zero correlation only measures linear dependence between variables, while independence is a much stronger condition that rules out any form of dependence.

### Part 2: Examples of Dependent Variables with Zero Correlation

Here are several examples where variables are dependent but uncorrelated:

1. **Quadratic Relationship:**
   Let $X \sim \text{Normal}(0,1)$ and $Y = X^2$

   - Clearly $Y$ depends on $X$
   - But $\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = E[X^3] - 0 = 0$
   - Therefore $\rho_{XY} = 0$ despite clear dependence

2. **Circular Relationship:**
   Let $X \sim \text{Uniform}(0,2\pi)$ and set:

   - $Y = \sin(X)$
   - These variables are dependent (Y is a function of X)
   - But $\text{Cov}(X,Y) = 0$ due to the periodic nature of sine

3. **Modified Joint Distribution:**
   Let $X$ and $Z$ be independent standard normal, then set:
   - $Y = ZX/|X|$
   - $X$ and $Y$ are uncorrelated but dependent through the sign relationship

### Key Insights

1. For normal random variables uniquely, zero correlation implies independence.

2. In general, the relationship between correlation and independence is:

   - Independence ⟹ Zero Correlation
   - Zero Correlation ⟹̸ Independence

3. Correlation only captures linear relationships between variables. Other forms of dependence (quadratic, periodic, etc.) can exist even when correlation is zero.

4. This highlights why testing for independence should not rely solely on correlation coefficients except in special cases like the multivariate normal distribution.

## Another Solution:

### 1. Initial Setup

First, we define our random variables:

1. Let $X \sim \text{Normal}(0,1)$ be a standard normal random variable

2. Define $Z$ as a random sign with probability mass function:

   $P(Z = -1) = 0.5$

   $P(Z = 1) = 0.5$

3. Construct $Y = ZX$

### 2. Proving Y is Standard Normal

To show $Y \sim \text{Normal}(0,1)$, we use the cumulative distribution function approach:

$P(Y < x) = P(ZX < x|Z = 1)P(Z = 1) + P(ZX < x|Z = -1)P(Z = -1)$

Let's break this down:

- When $Z = 1$: $ZX = X$, so $P(ZX < x|Z = 1) = P(X < x)$
- When $Z = -1$: $ZX = -X$, so $P(ZX < x|Z = -1) = P(-X < x) = P(X > -x)$

Therefore:

$P(Y < x) = P(X < x)(0.5) + P(-X < x)(0.5)$

$= (0.5)(P(X < x) + P(X \geq -x))$

$= (0.5)(P(X < x) + P(X < x))$ (by symmetry of normal distribution)

$= P(X < x)$

This proves that $Y$ has exactly the same distribution as $X$, namely $\text{Normal}(0,1)$.

### 3. Proving Zero Correlation

Now we show that $X$ and $Y$ have zero correlation:

$\rho = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y} = \text{Cov}(X,Y)$ (since both variances are 1)

$= \text{Cov}(X,ZX)$

$= E(XZX) - E(X)E(ZX)$

$= E(X^2Z) - E(X)E(Z)E(X)$ (by independence of $X$ and $Z$)

$= E(X^2)E(Z) - 0$

$= 0$ (since $E(Z) = 0$)

### 4. Understanding the Dependence

Despite having zero correlation, $X$ and $Y$ are clearly dependent because:

- $Y$ is completely determined by $X$ and $Z$
- For any given value of $X$, $Y$ can only take on two possible values: $X$ or $-X$
- This is a perfect example of non-linear dependence that correlation fails to capture

### Key Insights

1. This construction shows that even for normal random variables, zero correlation doesn't always imply independence.

2. The dependence is created through a clever sign-switching mechanism that preserves:

   - The marginal normal distribution
   - Zero correlation
   - Clear dependence structure

3. This example is particularly useful because:
   - It's mathematically tractable
   - The dependence is easy to understand intuitively
   - All calculations can be done exactly rather than approximately

This construction serves as a powerful reminder that correlation only measures linear relationships between variables, and we must be careful not to equate zero correlation with independence, even in seemingly simple cases.

First, we define our random variables:

1. Let $X \sim \text{Normal}(0,1)$ be a standard normal random variable

2. Define $Z$ as a random sign with probability mass function:

   $P(Z = -1) = 0.5$
   $P(Z = 1) = 0.5$

3. Construct $Y = ZX$

### 2. Proving Y is Standard Normal

To show $Y \sim \text{Normal}(0,1)$, we use the cumulative distribution function approach:

$P(Y < x) = P(ZX < x|Z = 1)P(Z = 1) + P(ZX < x|Z = -1)P(Z = -1)$

Let's break this down:

- When $Z = 1$: $ZX = X$, so $P(ZX < x|Z = 1) = P(X < x)$
- When $Z = -1$: $ZX = -X$, so $P(ZX < x|Z = -1) = P(-X < x) = P(X > -x)$

Therefore:

$P(Y < x) = P(X < x)(0.5) + P(-X < x)(0.5)$

$= (0.5)(P(X < x) + P(X \geq -x))$

$= (0.5)(P(X < x) + P(X < x))$ (by symmetry of normal distribution)

$= P(X < x)$

This proves that $Y$ has exactly the same distribution as $X$, namely $\text{Normal}(0,1)$.

### 3. Proving Zero Correlation

Now we show that $X$ and $Y$ have zero correlation:

$\rho = \frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y} = \text{Cov}(X,Y)$ (since both variances are 1)

$= \text{Cov}(X,ZX)$

$= E(XZX) - E(X)E(ZX)$

$= E(X^2Z) - E(X)E(Z)E(X)$ (by independence of $X$ and $Z$)

$= E(X^2)E(Z) - 0$

$= 0$ (since $E(Z) = 0$)

### 4. Understanding the Dependence

Despite having zero correlation, $X$ and $Y$ are clearly dependent because:

- $Y$ is completely determined by $X$ and $Z$
- For any given value of $X$, $Y$ can only take on two possible values: $X$ or $-X$
- This is a perfect example of non-linear dependence that correlation fails to capture

### Key Insights

1. This construction shows that even for normal random variables, zero correlation doesn't always imply independence.

2. The dependence is created through a clever sign-switching mechanism that preserves:

   - The marginal normal distribution
   - Zero correlation
   - Clear dependence structure

3. This example is particularly useful because:
   - It's mathematically tractable
   - The dependence is easy to understand intuitively
   - All calculations can be done exactly rather than approximately

This construction serves as a powerful reminder that correlation only measures linear relationships between variables, and we must be careful not to equate zero correlation with independence, even in seemingly simple cases.

<img src="Code/Figures/Q6.png" alt="alt text">
