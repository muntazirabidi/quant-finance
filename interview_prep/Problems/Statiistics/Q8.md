# Linear Regression: Assumptions, Estimation, and Inference

## Model Specification

We begin with the linear regression model:

$Y = X\beta + \varepsilon$

where:

- $Y$ is an $(n \times 1)$ vector of dependent variables
- $X$ is an $(n \times k)$ matrix of independent variables
- $\beta$ is a $(k \times 1)$ vector of unknown parameters
- $\varepsilon$ is an $(n \times 1)$ vector of error terms

## Classical Linear Regression Assumptions

### 1. Linearity

The relationship between $X$ and $Y$ must be linear in parameters. Mathematically:

$E[Y|X] = X\beta$

This means the expected value of $Y$ given $X$ is a linear function of $X$. Note that the variables themselves can be nonlinear transformations (like $X^2$ or $\ln(X)$), but the parameters $\beta$ must enter linearly.

### 2. Random Sampling

The observations $(Y_i, X_i)$ are independently and identically distributed (i.i.d.) draws from their joint distribution. This ensures that:

$\text{Cov}(Y_i, Y_j|X) = 0$ for all $i \neq j$

### 3. No Perfect Multicollinearity

The matrix $X$ must have full column rank, meaning:

$\text{rank}(X) = k$

This ensures that $(X'X)^{-1}$ exists and the parameters can be uniquely identified.

### 4. Strict Exogeneity

The error term has zero conditional mean:

$E[\varepsilon|X] = 0$

This implies that the regressors are exogenous and contain no information about the error term.

### 5. Homoskedasticity

The error term has constant variance conditional on $X$:

$\text{Var}(\varepsilon|X) = \sigma^2I_n$

where $I_n$ is the $(n \times n)$ identity matrix.

### 6. Normality (for inference)

For conducting hypothesis tests and constructing confidence intervals:

$\varepsilon|X \sim N(0, \sigma^2I_n)$

## Parameter Estimation

### Ordinary Least Squares (OLS)

The OLS estimator minimizes the sum of squared residuals:

$\min_{\beta} (Y - X\beta)'(Y - X\beta)$

Taking the first-order condition and solving yields the OLS estimator:

$\hat{\beta} = (X'X)^{-1}X'Y$

Under the classical assumptions, this estimator has several important properties:

1. Unbiasedness:
   $E[\hat{\beta}|X] = \beta$

2. Efficiency (Gauss-Markov Theorem):
   $\hat{\beta}$ is the Best Linear Unbiased Estimator (BLUE)

3. Consistency:
   $\text{plim}_{n \to \infty} \hat{\beta} = \beta$

## Statistical Inference on $\beta$

### 1. Individual Coefficient Tests

For testing individual coefficients, we can use t-tests:

$t = \frac{\hat{\beta}_j - \beta_j^0}{\text{se}(\hat{\beta}_j)}$

where $\text{se}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2(X'X)^{-1}_{jj}}$ and $\hat{\sigma}^2 = \frac{e'e}{n-k}$

Under $H_0: \beta_j = \beta_j^0$, this follows a t-distribution with $(n-k)$ degrees of freedom.

### 2. Joint Hypothesis Testing

For testing multiple linear restrictions $R\beta = r$:

$F = \frac{(R\hat{\beta} - r)'[R(X'X)^{-1}R']^{-1}(R\hat{\beta} - r)/q}{e'e/(n-k)}$

This follows an F-distribution with $(q, n-k)$ degrees of freedom under $H_0$, where $q$ is the number of restrictions.

### 3. Confidence Intervals

For individual coefficients, $(1-\alpha)$ confidence intervals are:

$\hat{\beta}_j \pm t_{1-\alpha/2, n-k} \cdot \text{se}(\hat{\beta}_j)$

### 4. Goodness of Fit

The coefficient of determination:

$R^2 = 1 - \frac{e'e}{(Y-\bar{Y})'(Y-\bar{Y})}$

Adjusted $R^2$ accounts for model complexity:

$\bar{R}^2 = 1 - \frac{e'e/(n-k)}{(Y-\bar{Y})'(Y-\bar{Y})/(n-1)}$

## Diagnostic Tests

### 1. Heteroskedasticity Tests

- White test
- Breusch-Pagan test
- Goldfeld-Quandt test

### 2. Serial Correlation Tests

- Durbin-Watson test
- Breusch-Godfrey test

### 3. Normality Tests

- Jarque-Bera test
- Shapiro-Wilk test

### 4. Model Specification Tests

- Ramsey RESET test
- Link test
- CUSUM test for parameter stability

These tests help verify whether the classical assumptions hold and whether the model is correctly specified. When assumptions are violated, appropriate corrections (such as robust standard errors, GLS, or model respecification) may be necessary.
