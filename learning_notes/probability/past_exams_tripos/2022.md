# Question

To show that $G(a) \geq \sigma^2$ for all $a$ and determine the value of $a$ for which equality holds, we start by expanding $G(a)$:

$$
G(a) = E[(X - a)^2]
$$

Expanding the square and taking the expectation, we get:

$$
G(a) = E[X^2] - 2aE[X] + a^2
$$

Using the fact that $E[X] = \mu$ and $\text{Var}(X) = E[X^2] - \mu^2$, we substitute $E[X^2] = \sigma^2 + \mu^2$:

$$
G(a) = \sigma^2 + \mu^2 - 2a\mu + a^2
$$

This is a quadratic function in $a$:

$$
G(a) = a^2 - 2a\mu + (\mu^2 + \sigma^2)
$$

The minimum value of this quadratic function occurs at $a = \mu$. Substituting $a = \mu$ into $G(a)$:

$$
G(\mu) = E[(X - \mu)^2] = \sigma^2
$$

Thus, $G(a) \geq \sigma^2$ for all $a$, with equality when $a = \mu$.

For $H(a) = E[|X - a|]$, we express it in terms of the probability density function $f$:

$$
H(a) = \int_{-\infty}^{\infty} |x - a| f(x) \, dx
$$

Splitting the integral at $a$:

$$
H(a) = \int_{-\infty}^a (a - x) f(x) \, dx + \int_a^\infty (x - a) f(x) \, dx
$$

To find the value of $a$ that minimizes $H(a)$, we differentiate with respect to $a$:

$$
\frac{dH}{da} = \int_{-\infty}^a f(x) \, dx - \int_a^\infty f(x) \, dx
$$

Setting the derivative equal to zero:

$$
\int_{-\infty}^a f(x) \, dx = \int_a^\infty f(x) \, dx
$$

Since the total integral of $f(x)$is 1, this implies:

$$
\int_{-\infty}^a f(x) \, dx = \frac{1}{2}
$$

Thus, $H(a)$ is minimized when $a$ is the median of the distribution, satisfying $\int_{-\infty}^a f(x) \, dx = \frac{1}{2}$.

### Final Answer

For $G(a)$, equality holds when $a = \boxed{\mu}$. For $H(a)$, the minimizer $a$ satisfies $\int_{-\infty}^{a} f(x) \, dx = \frac{1}{2}$, so the answer is $ \boxed{\text{the median}}$.

# Question

- (a) Let $U$ and $V$ be two bounded random variables such that $E[U^k] = E[V^k]$ for all
  non-negative integers $k$. Show that $U$ and $V$ have the same moment generating function.

- (b) Let $X$ be a continuous random variable with probability density function

  $$
  f(x) = A e^{-x^2/2}
  $$

  for all real $x$, where A is a normalising constant. Compute the moment generating function of $X$.

- (c) Let $Y$ be a discrete random variable with probability mass function
  $$
  P(Y= n) = Be^{-n^2/2}
  $$
  for all integers $n$, where $B$ is a normalising constant. Show that
  $$
  E[e^{kY}] = E[e^{kX}]
  $$
  for all integers $k$, where $X$ is a standard normal random variable.

## Solution part a:

To show that $U$ and $V$ have the same moment generating function (MGF) given that their moments are equal, we proceed as follows:

**Step 1: Definition of MGF**  
The MGF of a random variable $X$ is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}].
$$

**Step 2: Taylor Series Expansion**  
Expand $e^{tX}$ as a power series:

$$
e^{tX} = \sum_{k=0}^{\infty} \frac{(tX)^k}{k!}.
$$

**Step 3: Linearity of Expectation**  
Take the expectation of both sides:

$$
\mathbb{E}[e^{tX}] = \mathbb{E}\left[\sum_{k=0}^{\infty} \frac{(tX)^k}{k!}\right].
$$

**Step 4: Interchange Expectation and Summation**  
Since $U$ and $V$ are bounded (say $|X| \leq C$ for $X = U$ or $V$), the series $\sum_{k=0}^{\infty} \frac{|tX|^k}{k!}$ is dominated by $e^{|t|C}$, which is integrable. By the Dominated Convergence Theorem:

$$
\mathbb{E}\left[\sum_{k=0}^{\infty} \frac{(tX)^k}{k!}\right] = \sum_{k=0}^{\infty} \frac{t^k \mathbb{E}[X^k]}{k!}.
$$

**Step 5: Equality of Moments**  
Given $\mathbb{E}[U^k] = \mathbb{E}[V^k]$ for all $k \geq 0$, substitute into the series:

$$
M_U(t) = \sum_{k=0}^{\infty} \frac{t^k \mathbb{E}[U^k]}{k!} = \sum_{k=0}^{\infty} \frac{t^k \mathbb{E}[V^k]}{k!} = M_V(t).
$$

**Conclusion**  
Thus, $U$ and $V$ have the same moment generating function.  
$\boxed{M_U(t) = M_V(t) \text{ for all } t \in \mathbb{R}}$

## Solution part b:

To compute the moment generating function (MGF) of the random variable $X$ with density $f(x) = A e^{-x^2/2}$, follow these steps:

**Step 1: Determine the Normalizing Constant $A$**  
The density must satisfy $\int_{-\infty}^\infty f(x) \, dx = 1$. Using the Gaussian integral:

$$
\int_{-\infty}^\infty e^{-x^2/2} \, dx = \sqrt{2\pi}.
$$

Thus, $A = \frac{1}{\sqrt{2\pi}}$.

**Step 2: Define the MGF**  
The MGF is given by:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^\infty e^{tx} f(x) \, dx = A \int_{-\infty}^\infty e^{tx - x^2/2} \, dx.
$$

**Step 3: Complete the Square in the Exponent**  
Rewrite the exponent:

$$
tx - \frac{x^2}{2} = -\frac{x^2 - 2tx}{2} = -\frac{(x - t)^2 - t^2}{2} = \frac{t^2}{2} - \frac{(x - t)^2}{2}.
$$

Substitute back into the integral:

$$
M_X(t) = A e^{t^2/2} \int_{-\infty}^\infty e^{-(x - t)^2/2} \, dx.
$$

**Step 4: Evaluate the Gaussian Integral**  
The integral $\int_{-\infty}^\infty e^{-(x - t)^2/2} \, dx$ equals $\sqrt{2\pi}$ (shift does not affect the value). Thus:

$$
M_X(t) = A e^{t^2/2} \cdot \sqrt{2\pi}.
$$

**Step 5: Substitute $A = \frac{1}{\sqrt{2\pi}}$**  
Simplify:

$$
M_X(t) = \frac{1}{\sqrt{2\pi}} \cdot e^{t^2/2} \cdot \sqrt{2\pi} = e^{t^2/2}.
$$

**Final Answer**  
The moment generating function of $X$ is:

$$
\boxed{M_X(t) = e^{t^2/2}}.
$$

## solution part (c):

To show that $\mathbb{E}[e^{kY}] = \mathbb{E}[e^{kX}]$ for all integers $k$, where $Y$ is a discrete random variable with PMF $P(Y = n) = B e^{-n^2/2}$ and $X$ is a standard normal random variable, follow these steps:

**Step 1: Normalizing Constant $B$**  
The PMF of $Y$ requires:

$$
\sum_{n=-\infty}^{\infty} P(Y = n) = B \sum_{n=-\infty}^{\infty} e^{-n^2/2} = 1.
$$

Thus, $B = \frac{1}{\sum_{n=-\infty}^{\infty} e^{-n^2/2}}$.

**Step 2: Compute $\mathbb{E}[e^{kY}]$**  
By definition:

$$
\mathbb{E}[e^{kY}] = \sum_{n=-\infty}^{\infty} e^{kn} P(Y = n) = B \sum_{n=-\infty}^{\infty} e^{kn - n^2/2}.
$$

**Step 3: Complete the Square in the Exponent**  
Rewrite the exponent:

$$
kn - \frac{n^2}{2} = -\frac{n^2 - 2kn}{2} = -\frac{(n - k)^2 - k^2}{2} = \frac{k^2}{2} - \frac{(n - k)^2}{2}.
$$

Substitute back:

$$
\mathbb{E}[e^{kY}] = B \sum_{n=-\infty}^{\infty} e^{\frac{k^2}{2} - \frac{(n - k)^2}{2}} = B e^{\frac{k^2}{2}} \sum_{n=-\infty}^{\infty} e^{-\frac{(n - k)^2}{2}}.
$$

**Step 4: Shift the Summation Index**  
Let $m = n - k$. The sum becomes:

$$
\sum_{m=-\infty}^{\infty} e^{-\frac{m^2}{2}} = \sum_{n=-\infty}^{\infty} e^{-\frac{n^2}{2}} = \frac{1}{B}.
$$

Thus:

$$
\mathbb{E}[e^{kY}] = B e^{\frac{k^2}{2}} \cdot \frac{1}{B} = e^{\frac{k^2}{2}}.
$$

**Step 5: Compare to $\mathbb{E}[e^{kX}]$**  
For a standard normal variable $X$, the MGF is:

$$
\mathbb{E}[e^{kX}] = e^{\frac{k^2}{2}}.
$$

**Conclusion**  
Since $\mathbb{E}[e^{kY}] = e^{\frac{k^2}{2}} = \mathbb{E}[e^{kX}]$ for all integers $k, we have shown:

$$
\boxed{\mathbb{E}[e^{kY}] = \mathbb{E}[e^{kX}] \quad \text{for all integers } k}.
$$

# Question2:

- (a) Let $X$ be a random variable valued in $\{1, 2, \dots\}$ and let $G-X$ be its probability generating
  function. Show that:

  $$
  P(X=k) = \frac{G_X^{(n)}(0)}{n!}
  $$

  where $G^{(n)}_X$ denotes the nth derivative of $G_X$.

- (b) Let $Y$ be another random variable valued in $\{1, 2, \dots\}$, independent of $X$. Prove that
  $G_{X+Y}(s) = G_X(s)G_Y(s)$ for all $0< s< 1$.
- (c) Compute $G_X$ in the case where $$ is a geometric random variable taking values in
  $\{1, 2, \dots\}$ with $P(X = 1) = p$ for a given constant $0 < p 1$.
- (d) A jar contains n marbles. Initially, all of the marbles are red. Every minute, a marble
  is drawn at random from the jar, and then replaced with a blue marble. Let T be the
  number of minutes until the jar contains only blue marbles. Compute the probability
  generating function $G_T$.

## Solution part (d):

#### **Step 1: Decompose the Process into Stages**

When $k$ red marbles remain ($k = n, n-1, \dots, 1$):

- The probability of drawing a red marble is $\frac{k}{n}$.
- The time $X_k$ to replace one red marble follows a **geometric distribution** with parameter $p = \frac{k}{n}$.

Thus, the total time $T$ is the sum of independent geometric variables:

$$
T = X_n + X_{n-1} + \dots + X_1,
$$

where $X_k \sim \text{Geometric}\left(\frac{k}{n}\right)$.

---

#### **Step 2: PGF of a Geometric Random Variable**

For $X \sim \text{Geometric}(p)$, the PGF is:

$$
G_X(s) = \frac{ps}{1 - (1 - p)s}.
$$

Applying this to $X_k$:

$$
G_{X_k}(s) = \frac{\frac{k}{n}s}{1 - \left(1 - \frac{k}{n}\right)s}.
$$

---

#### **Step 3: PGF of the Sum $T$**

Since $X_k$ are independent, the PGF of $T$ is the product of individual PGFs:

$$
G_T(s) = \prod_{k=1}^n G_{X_k}(s).
$$

Substituting $G\_{X_k}(s)$:

$$
G_T(s) = \prod_{k=1}^n \frac{\frac{k}{n}s}{1 - \left(1 - \frac{k}{n}\right)s}.
$$

---

#### **Step 4: Simplify the Product**

Factor the denominator:

$$
1 - \left(1 - \frac{k}{n}\right)s = \frac{n - (n - k)s}{n}.
$$

Substitute into $G_T(s)$:

$$
G_T(s) = \prod_{k=1}^n \frac{\frac{k}{n}s}{\frac{n - (n - k)s}{n}} = \prod_{k=1}^n \frac{ks}{n - (n - k)s}.
$$

---

#### **Final Result**

The probability generating function for $T$ is:

$$
\boxed{G_T(s) = \prod_{k=1}^n \frac{k s}{n - (n - k)s}}.
$$

---

### **Key Observations**

1. **Coupon Collector Analogy**:  
   $T$ is analogous to the coupon collector problem, where $T$ represents the time to collect all $n$ coupons (here, replacing all $n$ red marbles).

2. **Moments from the PGF**:

   - The expected time $\mathbb{E}[T]$ can be derived by evaluating $G_T'(1)$.
   - Higher moments (variance, skewness) require higher-order derivatives.

3. **Convergence**:  
   The PGF converges for $|s| < \frac{n}{n - k}$ for all $k$, ensuring the series is well-defined.

---

This solution leverages the properties of geometric distributions and the independence of stages to construct the PGF of $T$.
