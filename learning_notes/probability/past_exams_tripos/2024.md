# Poisson Sums and Probability Bounds

## Question

Let $X_1, X_2, ..., X_n$ be independent and identically distributed Poisson random variables with mean 1. Let $S = X_1 + \cdots + X_n$.

(a) Compute the moment generating function of $S$ and find its distribution.
(b) Prove that $P(S \geq 2n) \leq (e/4)^n$

## Solution

### Part (a) - Moment Generating Function and Distribution

Let's start by recalling a fundamental property of Poisson distributions: for a Poisson random variable $X$ with mean $\mu$, its moment generating function is:

$M_X(t) = \exp(\mu(e^t - 1))$

For our case, each $X_i \sim \text{Poisson}(1)$, so:
$M_{X_i}(t) = \exp(e^t - 1)$

For the sum $S$, we can use the key property that the MGF of a sum of independent random variables is the product of their individual MGFs:

$M_S(t) = M_{X_1}(t) \times M_{X_2}(t) \times \cdots \times M_{X_n}(t)$
$= [\exp(e^t - 1)]^n$
$= \exp(n(e^t - 1))$

This is immediately recognizable as the MGF of a Poisson distribution with mean $n$. Therefore:

$S \sim \text{Poisson}(n)$

### Part (b) - Probability Bound

To prove $P(S \geq 2n) \leq (e/4)^n$, we'll use an exponential bound combined with the MGF we just found.

1. From Markov's exponential inequality, for any $t > 0$:
   $P(S \geq 2n) \leq e^{-2nt}M_S(t)$

2. Substituting our MGF:
   $P(S \geq 2n) \leq e^{-2nt}\exp(n(e^t - 1))$
   $= \exp(n(e^t - 2t - 1))$

3. To minimize this bound, we differentiate the exponent with respect to $t$:
   $\frac{d}{dt}[n(e^t - 2t - 1)] = n(e^t - 2) = 0$

   This gives us $t = \ln(2)$

4. Substituting $t = \ln(2)$:
   $P(S \geq 2n) \leq \exp(n(2 - 2\ln(2) - 1))$
   $= \exp(n(1 - 2\ln(2)))$
   $= (e/4)^n$

### Key Insights

1. The sum of independent Poisson random variables is again Poisson with mean equal to the sum of the individual means. This is a special property not shared by many other distributions.

2. The exponential bound we used is often tighter than simpler bounds like Markov's or Chebyshev's inequalities, especially for tail probabilities.

3. The choice of $t = \ln(2)$ was optimal and gives us an exponentially decaying bound in $n$. This rapid decay reflects the strong concentration of measure for sums of independent random variables.

4. The bound $(e/4)^n$ is quite tight - since $e/4 \approx 0.68 < 1$, it shows that the probability of $S$ being twice its mean decreases exponentially with $n$.

# Question 2: Bivariate Normal Distribution Analysis

## Part (a) - Joint Probability Density Function

Let's recall the general form of a bivariate normal PDF and then substitute our specific parameters.

The joint PDF for a bivariate normal distribution is:

$f_{X_1,X_2}(x_1,x_2) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x_1-\mu_1)^2}{\sigma_1^2} - \frac{2\rho(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2}\right]\right)$

This formula might look intimidating, but we can understand its components:

- The denominator $2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}$ is a normalizing constant
- The exponential term contains the core of the bivariate relationship
- $\rho$ controls the correlation between $X_1$ and $X_2$
- When $\rho = 0$, the middle term in the exponential vanishes, giving us independent normal distributions

### Key Insights:

1. This density is symmetric in $x_1$ and $x_2$
2. The parameter $\rho$ must satisfy $|\rho| < 1$ for this to be a valid PDF
3. The level curves of this density are ellipses
4. When $\rho = 0$, the PDF factors into the product of two univariate normal densities

## Extension

If we wanted to understand how this relates to a real-world scenario, consider a weather model where:

- $X_1$ is the temperature
- $X_2$ is humidity
- $\rho$ represents how these variables are correlated
- The means $\mu_1$ and $\mu_2$ represent typical values
- The variances $\sigma_1^2$ and $\sigma_2^2$ represent the spread of values
