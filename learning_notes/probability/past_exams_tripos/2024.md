# Question1: Poisson Sums and Probability Bounds (3F)

Let $X_1, X_2, ..., X_n$ be independent and identically distributed Poisson random variables with mean 1. Let $S = X_1 + \cdots + X_n$.

(a) Compute the moment generating function of $S$ and find its distribution.
(b) Prove that $P(S \geq 2n) \leq (e/4)^n$

## Solution

### Part (a) - Moment Generating Function and Distribution

Let's start by recalling a fundamental property of Poisson distributions: for a Poisson random variable $X$ with mean $\mu$, its moment generating function is:

$M_X(t) = \exp(\mu(e^t - 1))$

For our case, each $X_i \sim \text{Poisson}(1)$, so:
$M_{X_i}(t) = \exp(e^t - 1)$

For the sum $S$, we can use the key property that the MGF of a sum of independent random variables is the product of their individual MGFs:

$M_S(t) = M_{X_1}(t) \times M_{X_2}(t) \times \cdots \times M_{X_n}(t)$
$= [\exp(e^t - 1)]^n$
$= \exp(n(e^t - 1))$

This is immediately recognizable as the MGF of a Poisson distribution with mean $n$. Therefore:

$S \sim \text{Poisson}(n)$

### Part (b) - Probability Bound

To prove $P(S \geq 2n) \leq (e/4)^n$, we'll use an exponential bound combined with the MGF we just found.

1. From Markov's exponential inequality, for any $t > 0$:
   $P(S \geq 2n) \leq e^{-2nt}M_S(t)$

2. Substituting our MGF:
   $P(S \geq 2n) \leq e^{-2nt}\exp(n(e^t - 1))$
   $= \exp(n(e^t - 2t - 1))$

3. To minimize this bound, we differentiate the exponent with respect to $t$:
   $\frac{d}{dt}[n(e^t - 2t - 1)] = n(e^t - 2) = 0$

   This gives us $t = \ln(2)$

4. Substituting $t = \ln(2)$:
   $P(S \geq 2n) \leq \exp(n(2 - 2\ln(2) - 1))$
   $= \exp(n(1 - 2\ln(2)))$
   $= (e/4)^n$

### Key Insights

1. The sum of independent Poisson random variables is again Poisson with mean equal to the sum of the individual means. This is a special property not shared by many other distributions.

2. The exponential bound we used is often tighter than simpler bounds like Markov's or Chebyshev's inequalities, especially for tail probabilities.

3. The choice of $t = \ln(2)$ was optimal and gives us an exponentially decaying bound in $n$. This rapid decay reflects the strong concentration of measure for sums of independent random variables.

4. The bound $(e/4)^n$ is quite tight - since $e/4 \approx 0.68 < 1$, it shows that the probability of $S$ being twice its mean decreases exponentially with $n$.

# Question 2: Bivariate Normal Distribution Analysis (4F)

## Part (a) - Joint Probability Density Function

Let's recall the general form of a bivariate normal PDF and then substitute our specific parameters.

The joint PDF for a bivariate normal distribution is:

$f_{X_1,X_2}(x_1,x_2) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x_1-\mu_1)^2}{\sigma_1^2} - \frac{2\rho(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2}\right]\right)$

This formula might look intimidating, but we can understand its components:

- The denominator $2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}$ is a normalizing constant
- The exponential term contains the core of the bivariate relationship
- $\rho$ controls the correlation between $X_1$ and $X_2$
- When $\rho = 0$, the middle term in the exponential vanishes, giving us independent normal distributions

### Key Insights:

1. This density is symmetric in $x_1$ and $x_2$
2. The parameter $\rho$ must satisfy $|\rho| < 1$ for this to be a valid PDF
3. The level curves of this density are ellipses
4. When $\rho = 0$, the PDF factors into the product of two univariate normal densities

## Extension

If we wanted to understand how this relates to a real-world scenario, consider a weather model where:

- $X_1$ is the temperature
- $X_2$ is humidity
- $\rho$ represents how these variables are correlated
- The means $\mu_1$ and $\mu_2$ represent typical values
- The variances $\sigma_1^2$ and $\sigma_2^2$ represent the spread of values

# Question 3: Minimum of Independent Exponential Random Variables

Let $S_1, S_2, ..., S_n$ be independent exponential random variables with means $E(S_i) = \frac{1}{q_i}$ for $i = 1,2,...,n$. Let $T = \min\{S_1, S_2, ..., S_n\}$ and let $K$ be the value of $i$ for which $S_i = T$.

We are asked to:

1. Find the distributions of random variables $K$ and $T$
2. Show that $K$ and $T$ are independent
3. Find the PDF of $X_n = \sum_{i=1}^n S_i$ where $S_1, S_2, ..., S_n$ are independent exponential random variables.

## Intuitive Understanding

Before diving into the mathematics, let's build some intuition. Imagine we have n lightbulbs, each with its own failure rate $q_i$. The time until each bulb fails follows an exponential distribution. We're interested in:

- Which bulb fails first (represented by $K$)
- How long until the first failure occurs (represented by $T$)

The exponential distribution has a special "memoryless" property: if a component has survived until time $t$, its future lifetime distribution is the same as if it were brand new. This unique property will be key to understanding why $K$ and $T$ are independent.

## Solution

### Part a: Distribution of K

To find $P(K = k)$, we consider the probability that the $k$th variable is the minimum:

$P(K = k) = \frac{q_k}{q_1 + q_2 + ... + q_n}$

This makes intuitive sense because:

- A higher rate parameter $q_k$ means a higher probability of being the minimum
- The denominator normalizes these probabilities so they sum to 1
- Each probability is proportional to its failure rate

### Part b: Distribution of T

The distribution of the minimum $T$ is:

$P(T \geq t) = e^{-(q_1 + q_2 + ... + q_n)t}$

This shows that $T$ follows an exponential distribution with rate parameter $\sum_{i=1}^n q_i$. This is logical because:

- The first failure can come from any of the $n$ components
- Each component contributes its failure rate to the overall rate
- The minimum of exponential random variables is also exponential

### Independence of K and T

To prove independence, we show that:

$P(K = k, T \geq t) = P(K = k) \times P(T \geq t)$

Let's verify:

1. From our earlier work: $P(K = k, T \geq t) = e^{-q_kt} \times \frac{q_k}{q_1 + q_2 + ... + q_n}$
2. $P(K = k) \times P(T \geq t) = \frac{q_k}{q_1 + q_2 + ... + q_n} \times e^{-(q_1 + q_2 + ... + q_n)t}$
3. These are equal, proving independence

## Significance

This independence result is remarkable because it tells us that:

- Knowing which component failed first gives us no information about when it failed
- Knowing when the first failure occurred tells us nothing about which component failed

This is a special property of exponential distributions and wouldn't hold for most other probability distributions. The result relies heavily on the memoryless property of exponential distributions.

Let me help you solve this step by step. I notice the question appears to be truncated - it seems to be asking about finding the probability density function (PDF) of the sum of n independent exponential random variables.

Let's approach this systematically:

### Part c: Sum of Independent Exponential Random Variables

Let's prove that when we sum $n$ independent exponential random variables with rate parameter $1$, the resulting probability density function is:

$$f_{X_n}(x) = \frac{x^{n-1}e^{-x}}{(n-1)!}, \quad x > 0$$

where $X_n = \sum_{i=1}^n S_i$ and each $S_i$ is an independent exponential random variable with rate $q_i = 1$ (mean 1).

### 1. Starting with Convolution

When we add independent random variables, their probability density functions are combined through convolution. For each individual exponential variable $S_i$, we have:

$$f_{S_i}(x) = e^{-x}, \quad x > 0$$

Let's first understand what happens when we add just two variables. The convolution formula gives us:

$$f_{S_1+S_2}(x) = \int_0^x f_{S_1}(t)f_{S_2}(x-t) dt$$

Substituting the exponential densities:

$$f_{S_1+S_2}(x) = \int_0^x e^{-t}e^{-(x-t)} dt = \int_0^x e^{-x} dt = xe^{-x}, \quad x > 0$$

### 2. The Inductive Step

Now we'll extend this to $n$ variables using mathematical induction. Let's assume that for $n-1$ variables, we have:

$$f_{X_{n-1}}(x) = \frac{x^{n-2}e^{-x}}{(n-2)!}, \quad x > 0$$

To find $f_{X_n}(x)$, we convolve this with another exponential density:

$f_{X_n}(x) = \int_0^x f_{X_{n-1}}(t)f_{S_n}(x-t) dt$

$= \int_0^x \frac{t^{n-2}e^{-t}}{(n-2)!}e^{-(x-t)} dt$

$= \frac{e^{-x}}{(n-2)!}\int_0^x t^{n-2} dt$

### 3. Completing the Proof

The integral evaluates to:

$\int_0^x t^{n-2} dt = \frac{x^{n-1}}{n-1}$

Therefore:

$$f_{X_n}(x) = \frac{e^{-x}}{(n-2)!} \cdot \frac{x^{n-1}}{n-1} = \frac{x^{n-1}e^{-x}}{(n-1)!}, \quad x > 0$$

### 4. Verification

We can verify this is a valid PDF by checking:

1. Non-negativity: Clear since $x > 0$ and all terms are positive
2. Integration to 1: Using the Gamma function:

   $$\int_0^\infty \frac{x^{n-1}e^{-x}}{(n-1)!} dx = \frac{\Gamma(n)}{(n-1)!} = 1$$

We have proven that the sum of $n$ independent exponential random variables with rate 1 follows a Gamma distribution with shape parameter $n$ and scale parameter 1. This elegant result showcases how the complexity of the distribution grows with each additional variable while maintaining a beautifully structured form.

> We used convolution here because $X_n = \sum^n_{i=1} S_i$ represents the **sum of independent random variables**, and the **distribution of a sum of independent random variables** can be derived using the convolution of their **individual probability density functions (PDFs)**.
>
> Convolution is a standard mathematical tool to compute the resulting distribution of a sum of independent random variables.

# Question 4: The Collector's Problem: Rolling a Die Until All Faces Appear

Let $T_n$ represent the minimum number of rolls required until all $n$ faces of an $n$-sided fair die appear at least once. We'll analyze various properties of $T_n$, including its expected value, variance, and limiting behavior.

## Part (i): Decomposition into Geometric Random Variables

**Show that $T_n$ can be expressed as the sum of $n$ independent geometric random variables.**

### Solution and Intuition

Let's break this down into stages to understand how we can collect all faces:

1. First, let's think about how the collection process works:

   - Stage 1: We roll until we see any face (guaranteed success on first roll)
   - Stage 2: We roll until we see a new face (not the one we already have)
   - Stage 3: We roll until we see another new face (not the two we already have)
     And so on...

2. For each stage $k$ (where $k$ goes from 1 to $n$):

   - We have already seen $k-1$ faces
   - We still need to see one of the remaining $n-(k-1)$ faces
   - The probability of success in this stage is:
     $p_k = \frac{n-k+1}{n}$

3. In each stage $k$, we're essentially waiting for a success with probability $p_k$, which follows a geometric distribution. Let's call these random variables:
   $X_k \sim \text{Geometric}(p_k)$

4. Therefore:
   $T_n = X_1 + X_2 + ... + X_n$

The independence of these geometric variables comes from the fact that each stage starts fresh, independent of how many rolls it took in previous stages.

## Part (ii): Expected Value

**Find $E[T_n]$**

### Solution and Intuition

1. For a geometric random variable with success probability $p$:
   $E[X] = \frac{1}{p}$

2. Using linearity of expectation:
   $E[T_n] = \sum_{k=1}^n E[X_k] = \sum_{k=1}^n \frac{n}{n-k+1} = n\sum_{k=1}^n \frac{1}{k} = nH_n$

   where $H_n$ is the $n$th harmonic number.

3. For large $n$, we can approximate:
   $H_n \approx \ln(n) + \gamma$
   where $\gamma \approx 0.5772$ is the Euler-Mascheroni constant.

Therefore:
$E[T_n] \approx n(\ln(n) + \gamma)$

**Intuition**: Why does it take $n\ln(n)$ rolls on average?

- The early faces are easy to find (high probability)
- As we collect more faces, finding a new one becomes increasingly difficult
- The logarithmic factor accounts for this increasing difficulty
- The last few faces are the hardest to find, dominating the total time

## Part (iii): Variance Bound

**Show that $\text{Var}(T_n) \leq Cn^2$ where $C = \sum_{i=1}^{\infty} \frac{1}{i^2} = \frac{\pi^2}{6}$**

### Solution and Intuition

1. For a geometric random variable:
   $\text{Var}(X_k) = \frac{1-p_k}{p_k^2}$

2. Substituting $p_k = \frac{n-k+1}{n}$:
   $\text{Var}(X_k) = \frac{n(k-1)}{(n-k+1)^2}$

3. Using independence:
   $\text{Var}(T_n) = \sum_{k=1}^n \text{Var}(X_k) \leq n^2\sum_{k=1}^n \frac{1}{k^2}$

4. Since $\sum_{k=1}^{\infty} \frac{1}{k^2} = \frac{\pi^2}{6}$:
   $\text{Var}(T_n) \leq \frac{\pi^2}{6}n^2$

**Intuition**: The variance is quadratic in $n$ because:

- Later stages have higher variance due to lower success probabilities
- These high-variance stages dominate the total variance
- The quadratic bound reflects the worst-case scenario

## Part (iv): Limiting Behavior

**Show that for any $\epsilon > 0$:**
$\lim_{n \to \infty} P(\frac{T_n}{n\ln n} - 1 > \epsilon) = 0$

### Solution and Intuition

1. Define $Z_n = \frac{T_n}{n\ln n} - 1$

2. Using Chebyshev's inequality:
   $P(|T_n - E[T_n]| > t) \leq \frac{\text{Var}(T_n)}{t^2}$

3. Apply variance bound and substitute appropriate $t$:
   $P(Z_n > \epsilon) \leq \frac{\pi^2}{6\epsilon^2(\ln n)^2}$

4. As $n \to \infty$, this probability approaches 0.

**Intuition**: This result shows that:

- $T_n$ is very close to $n\ln n$ for large $n$
- The ratio $\frac{T_n}{n\ln n}$ converges to 1 in probability
- This confirms our earlier intuition about the $n\ln n$ scaling

This is a beautiful example of how a seemingly simple collecting problem reveals rich mathematical structure and connects to fundamental concepts like harmonic numbers and the Euler-Mascheroni constant.

# Question 6: Random Walk Probability Question

Let $(S_n : n \geq 0)$ be a simple random walk on $\mathbb{Z}$ with $S_0 = 0$ and $P(S_n - S_{n-1} = 1) = p$ and $P(S_n - S_{n-1} = -1) = q = 1-p$ for all $n \geq 1$.

### (i)

Find the distribution of $S_n$.

### (ii)

Find $b_n, c_n$ so that

$$\mathbb{P}\left(\frac{S_n - b_n}{c_n} \leq x\right) \to \Phi(x)$$

as $n \to \infty$, where $\Phi$ is the standard normal distribution function.

[You may quote standard results from lectures.]

From now on, assume that $p = q = 1/2$.

### (iii)

Let $T$ be the random number of steps taken by the random walk until it first hits $-a$ or $b$ for some $a,b \in \mathbb{N}$. Find $\mathbb{E}(T)$.

### (iv)

Let $V_n$ be the number of visits to the origin until time $n$, that is,

$$V_n = |\{0 \leq i \leq n: S_i = 0\}|$$

Using Stirling's formula or otherwise, prove that there exists some $c > 0$ such that

$$\mathbb{E}(V_{2n}) \geq c\sqrt{n}$$

for all $n$.

## Solution Part (i): Distribution of $S_n$

Let's find the distribution of $S_n$ for a simple random walk where $P(S_n - S_{n-1} = 1) = p$ and $P(S_n - S_{n-1} = -1) = q = 1-p$.

We observe that $S_n = \sum_{i=1}^n X_i$, where $X_i$ are independent and identically distributed random variables taking values:

- $X_i = 1$ with probability $p$
- $X_i = -1$ with probability $q = 1-p$

To reach position $m$ after $n$ steps:

- We need $(n+m)/2$ steps up (+1) and $(n-m)/2$ steps down (-1)
- This is only possible if $n+m$ is even and $|m| \leq n$
- The number of up steps follows a binomial distribution

Therefore, for values of $m$ where $-n \leq m \leq n$ and $m+n$ is even:

$$P(S_n = m) = \binom{n}{\frac{n+m}{2}} p^{\frac{n+m}{2}} q^{\frac{n-m}{2}}$$

For all other values of $m$:
$$P(S_n = m) = 0$$

## Solution Part (ii): Normal Approximation

To find $b_n$ and $c_n$ such that $\frac{S_n - b_n}{c_n} \to N(0,1)$, we use the Central Limit Theorem.

For each $X_i$:

- $E(X_i) = p \cdot 1 + q \cdot (-1) = p - q = 2p-1 = \mu$
- $Var(X_i) = E(X_i^2) - (E(X_i))^2 = 1 - (p-q)^2 = 4pq = \sigma^2$

Therefore:

- $E(S_n) = n\mu = n(2p-1)$
- $Var(S_n) = n\sigma^2 = 4npq$

By the Central Limit Theorem:

$$\frac{S_n - n\mu}{\sigma\sqrt{n}} \to N(0,1)$$

Therefore:
$$b_n = n(2p-1)$$
$$c_n = 2\sqrt{npq}$$

## Solution Part (iii): Expected Hitting Time for Symmetric Random Walk

Let's find $E(T)$ where $T$ is the first hitting time of either $-a$ or $b$ for a symmetric random walk ($p = q = \frac{1}{2}$).

Let's define $h(x)$ as the expected hitting time starting from position $x$:
$$h(x) = E(T | S_0 = x)$$

For $-a < x < b$, we can write:
$$h(x) = 1 + \frac{1}{2}h(x+1) + \frac{1}{2}h(x-1)$$

This equation comes from:

- One step must be taken (adding 1 to the time)
- With probability $\frac{1}{2}$, we move to $x+1$ and need $h(x+1)$ more steps
- With probability $\frac{1}{2}$, we move to $x-1$ and need $h(x-1)$ more steps

### Boundary Conditions

At the boundaries:
$$h(-a) = h(b) = 0$$

Since these are the target points where the walk stops.

### Solving the Difference Equation

The equation $h(x) = 1 + \frac{1}{2}h(x+1) + \frac{1}{2}h(x-1)$ rearranges to:
$$h(x+1) - 2h(x) + h(x-1) = -2$$

This is a second-order linear difference equation. The general solution is:
$$h(x) = Ax + B - x^2$$

Using the boundary conditions:

- $h(-a) = -Aa + B - a^2 = 0$
- $h(b) = Ab + B - b^2 = 0$

Solving these equations:
$$A = a + b$$
$$B = ab$$

Therefore:
$$h(x) = (a+b)x + ab - x^2$$

### Final Answer

Starting from $x = 0$:
$$E(T) = h(0) = ab$$

This means the expected number of steps until hitting either $-a$ or $b$, starting from 0, is equal to the product of the distances to both barriers.

## Solution Part (iv): Expected Number of Returns to Origin

Let's prove that $E(V_{2n}) \geq c\sqrt{n}$ for some constant $c > 0$, where $V_{2n}$ counts visits to 0 up to time $2n$.

For the symmetric random walk ($p = q = \frac{1}{2}$), we can write:

$$E(V_{2n}) = \sum_{k=0}^{2n} P(S_k = 0)$$

Note that $S_k$ can be 0 only when $k$ is even, so:

$$E(V_{2n}) = \sum_{m=0}^n P(S_{2m} = 0)$$

For a symmetric random walk, we know that:

$$P(S_{2m} = 0) = \binom{2m}{m} \left(\frac{1}{2}\right)^{2m}$$

### Applying Stirling's Formula

Stirling's formula states that as $m \to \infty$:

$$m! \sim \sqrt{2\pi m} \left(\frac{m}{e}\right)^m$$

Therefore:

$$\binom{2m}{m} = \frac{(2m)!}{(m!)^2} \sim \frac{\sqrt{4\pi m}(2m/e)^{2m}}{2\pi m(m/e)^{2m}} = \frac{2^{2m}}{\sqrt{\pi m}}$$

Thus:

$$P(S_{2m} = 0) = \binom{2m}{m} \left(\frac{1}{2}\right)^{2m} \sim \frac{1}{\sqrt{\pi m}}$$

Therefore:

$$E(V_{2n}) = \sum_{m=0}^n P(S_{2m} = 0) \sim \sum_{m=1}^n \frac{1}{\sqrt{\pi m}}$$

The sum of terms $\frac{1}{\sqrt{m}}$ from 1 to $n$ is bounded below by:

$$\sum_{m=1}^n \frac{1}{\sqrt{m}} \geq \int_1^n \frac{dx}{\sqrt{x}} = 2(\sqrt{n} - 1)$$

Therefore, there exists some constant $c > 0$ (we can take $c = \frac{1}{\sqrt{\pi}}$) such that:

$$E(V_{2n}) \geq c\sqrt{n}$$

for all sufficiently large $n$. By adjusting $c$ if necessary, we can make this hold for all $n \geq 1$.

## What do we learn from this result:

The key finding that $E(V_{2n}) \geq c\sqrt{n}$ tells us several important things:

First, it shows that a symmetric random walk returns to its starting point (the origin) remarkably often. The expected number of returns grows at least as fast as the square root of time. This is quite counterintuitive - even though the walk can wander arbitrarily far from the origin, it keeps coming back with high frequency.

This leads to a fundamental property called "recurrence" of the one-dimensional random walk. The fact that the expected number of visits grows without bound implies that the walk will return to the origin infinitely often with probability 1. This is a special property of one and two dimensional random walks - in three or more dimensions, random walks are "transient" and eventually wander away forever.

The $\sqrt{n}$ growth rate also reveals something deeper about the spatial spread of random walks. By time n, the typical distance from the origin is on the order of $\sqrt{n}$ (this follows from the Central Limit Theorem we saw earlier). So the walk spends a significant fraction of its time near its typical displacement - it's not just making rare excursions to that distance.

This has important applications in physics and chemistry. For example, the motion of particles undergoing Brownian motion follows a random walk pattern. Our result helps explain diffusion processes - how particles spread out over time and how often they return to their starting position. This has implications for chemical reaction rates, heat conduction, and many other physical phenomena.

In economics and finance, random walks are used to model stock prices and other financial quantities. The recurrence property suggests that prices will repeatedly return to previous levels, though the timing is unpredictable.

So this seemingly abstract mathematical result about random walks gives us deep insights into the behavior of many real-world systems that involve random motion or fluctuations. It's a beautiful example of how probability theory can reveal universal patterns in nature.
